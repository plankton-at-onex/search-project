{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import math\n",
    "import time\n",
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    response = openai.Embedding.create(\n",
    "    input=input,\n",
    "    model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "def cosine_dist(arr1, arr2):\n",
    "    # Given that arr1 and arr2 are non-normalized vectors\n",
    "    return 1 - np.dot(arr1,arr2)/(norm(arr1)*norm(arr2))\n",
    "\n",
    "def cosine_dist_norm(arr1, arr2):\n",
    "    # Given that arr1 and arr2 are normalized vectors\n",
    "    return 1 - np.dot(arr1,arr2)\n",
    "\n",
    "def euclidean_dist(arr1, arr2):\n",
    "    # Computes the Euclidean distance between two vectors arr1 and arr2.\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    return np.sqrt(np.dot(arr1 - arr2, arr1 - arr2))\n",
    "\n",
    "def manhattan_dist(arr1, arr2):\n",
    "    # Computes the L1 distance (Manhattan distance) between two vectors arr1 and arr2.\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    return np.sum(np.abs(arr1 - arr2))\n",
    "\n",
    "def dist(type, arr1, arr2):\n",
    "    if type.lower()[:3] == 'cos':\n",
    "        return cosine_dist(arr1, arr2)\n",
    "    elif type.lower()[:3] == 'euc':\n",
    "        return euclidean_dist(arr1, arr2)\n",
    "    elif type.lower()[:3] == 'man':\n",
    "        return manhattan_dist(arr1, arr2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sheets(df, info):\n",
    "\n",
    "    # Create page count df\n",
    "    id_count = df.groupby('ID').count().to_dict()['Embedding']\n",
    "    df = pd.DataFrame([id_count]).transpose().reset_index()\n",
    "    df.columns = ['ID', 'Count']\n",
    "\n",
    "    info = info[info['ID'].isin(id_count.keys())]\n",
    "    info = info.reset_index().drop(columns=['index'])\n",
    "\n",
    "    pages_df = pd.merge(info, df, how ='inner', on = 'ID')\n",
    "    pages_df.drop(pages_df[pages_df['Count'] == 0].index)\n",
    "    pages_df['Scientific Name'] = pages_df['Scientific Name'].apply(lambda x: x.split(', '))\n",
    "    pages_df['Specialty'] = pages_df['Specialty'].apply(lambda x: x.split(', '))\n",
    "    \n",
    "    # count occurrences of Scientific Name\n",
    "    scientific_name_counts = {}\n",
    "    for names in pages_df['Scientific Name']:\n",
    "        for name in names:\n",
    "            if name not in scientific_name_counts:\n",
    "                scientific_name_counts[name] = 1\n",
    "            else:\n",
    "                scientific_name_counts[name] += 1\n",
    "\n",
    "    # count occurrences of Specialty\n",
    "    specialty_counts = {}\n",
    "    for specialties in pages_df['Specialty']:\n",
    "        for specialty in specialties:\n",
    "            if specialty not in specialty_counts:\n",
    "                specialty_counts[specialty] = 1\n",
    "            else:\n",
    "                specialty_counts[specialty] += 1\n",
    "\n",
    "    # create new dataframes with the counts\n",
    "    scientific_name_df = pd.DataFrame.from_dict(scientific_name_counts, orient='index', columns=['Count'])\n",
    "    specialty_df = pd.DataFrame.from_dict(specialty_counts, orient='index', columns=['Count'])\n",
    "\n",
    "    # add column to indicate specialty or scientific name\n",
    "    scientific_name_df['Type'] = 'Scientific Name'\n",
    "    specialty_df['Type'] = 'Specialty'\n",
    "\n",
    "    # merge the two dataframes\n",
    "    sp_sn_count = pd.concat([scientific_name_df, specialty_df], axis=0, sort=False)\n",
    "    sp_sn_count = sp_sn_count.reset_index()\n",
    "    sp_sn_count.columns = ['Word', 'Count', 'Type']\n",
    "    sp_sn_count = sp_sn_count.sort_values('Count', ascending=False)\n",
    "    \n",
    "    return pages_df, sp_sn_count.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_embedding_dimensionality(df, num_components):\n",
    "    df_temp = df.copy()\n",
    "    # Extract the embeddings from the 'Embedding' column of the DataFrame\n",
    "    embeddings = np.array(df_temp['Embedding'].tolist())\n",
    "    \n",
    "    # Instantiate a PCA object with the desired number of components\n",
    "    pca = PCA(n_components=num_components)\n",
    "    \n",
    "    # Fit the PCA model to the embeddings\n",
    "    pca.fit(embeddings)\n",
    "    \n",
    "    # Transform the embeddings to their reduced representation\n",
    "    reduced_embeddings = pca.transform(embeddings)\n",
    "    \n",
    "    # Add the reduced embeddings to a new column in the DataFrame\n",
    "    df_temp[f'Embedding - {num_components}'] = reduced_embeddings.tolist()\n",
    "    df_temp[f'Embedding - {num_components}'] = df_temp[f'Embedding - {num_components}'].apply(lambda x: np.array(x))\n",
    "    df_temp = df_temp.drop(columns = ['Embedding'])\n",
    "    \n",
    "    return df_temp, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('openai_concat_embed_sample_smaller.csv')\n",
    "df = df.dropna(subset=['Embedding'])\n",
    "df['Embedding'] = df['Embedding'].apply(lambda x: \n",
    "                                    np.fromstring(\n",
    "                                        x.replace('[','')\n",
    "                                        .replace(']','')\n",
    "                                        .replace('  ',' '), sep=', '))\n",
    "\n",
    "info = pd.read_csv('Google Sheets.csv').set_index('ID')\n",
    "info['Scientific Name'] = info['Scientific Name'].apply(lambda x: x.split(', '))\n",
    "info['Specialty'] = info['Specialty'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_MIN = 700\n",
    "DIM_MAX = 1300\n",
    "DIM_STEP = 200\n",
    "\n",
    "sp_prompts = ['Rehabilitation', 'Cardiology', 'Woundcare & Dressing', 'Medical Clothing', 'Pediatrics',\n",
    "              'Monitoring', 'Pharmaceutical', 'Urology & Nephrology', 'Gastroenterology', 'Rapid Tests']\n",
    "\n",
    "sn_prompts = ['Vitamin C', 'Shockwave Therapy Machines', 'Sunscreen']\n",
    "\n",
    "dims = [1536] + (list(range(DIM_MAX, DIM_MIN - 1, -DIM_STEP)))\n",
    "\n",
    "pres = [np.float64, np.float32, np.float16]\n",
    "# dists = ['Cosine', 'Euclidean', 'Manhattan']\n",
    "df_log = pd.DataFrame(columns = ['Dimensions', 'Precision', 'Distance', 'Prompt', 'Top 5', 'Top 10', 'Top 20', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds = {}\n",
    "for p in (sp_prompts + sn_prompts):\n",
    "    prompt_embeds[p] = embed(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_finds = {}\n",
    "for p in sp_prompts:\n",
    "    should_finds[p] = info[info['Specialty'].apply(lambda x: p in x)].index.tolist()\n",
    "for p in sn_prompts:\n",
    "    should_finds[p] = info[info['Scientific Name'].apply(lambda x: p in x)].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = df.copy()\n",
    "for p in pres:\n",
    "    df_pre = df_red.copy()\n",
    "    df_pre[f'Embedding'] = df_pre[f'Embedding'].apply(lambda x: x.astype(p))\n",
    "    for dis in ['Cosine', 'Euclidean', 'Manhattan']:\n",
    "        for prompt in (sp_prompts + sn_prompts):\n",
    "            prompt_embed = prompt_embeds[prompt]\n",
    "\n",
    "            start = time.time()\n",
    "            distances = []\n",
    "            for i, row in df_pre.iterrows():\n",
    "                distances.append(dist(dis, prompt_embed, row[f'Embedding']))\n",
    "\n",
    "            closest_indices = np.argsort(distances)\n",
    "\n",
    "            closest_rows = df_pre.iloc[closest_indices].values.tolist()\n",
    "\n",
    "            mfrs_returned = []\n",
    "            for m in [r[0] for r in closest_rows]:\n",
    "                if m not in mfrs_returned:\n",
    "                    mfrs_returned.append(m)\n",
    "            # mfrs_found = [id for id in mfrs_returned if id in should_finds[prompt]]\n",
    "            # mfrs_new = [id for id in mfrs_returned if id not in should_finds[prompt]]\n",
    "            # mfrs_missed = [id for id in should_finds[prompt] if id not in mfrs_returned]\n",
    "            \n",
    "            mfrs_top_5 =  len([id for id in mfrs_returned[:5] if id in should_finds[prompt]]) / 5\n",
    "            mfrs_top_10 = len([id for id in mfrs_returned[:10] if id in should_finds[prompt]]) / 10\n",
    "            mfrs_top_20 = len([id for id in mfrs_returned[:20] if id in should_finds[prompt]]) / 20\n",
    "\n",
    "            end = time.time() - start\n",
    "            \n",
    "            df_to_concat = pd.DataFrame({\n",
    "                'Dimensions' : [d],\n",
    "                'Precision' : [str(p).split(\"'\")[1]],\n",
    "                'Distance' : [dis],\n",
    "                'Prompt' : [prompt],\n",
    "                'Top 5' : [mfrs_top_5],\n",
    "                'Top 10' : [mfrs_top_10],\n",
    "                'Top 20' : [mfrs_top_20],\n",
    "                'Time' : [round(end, 3)]\n",
    "            })\n",
    "            df_log = pd.concat([df_log, df_to_concat], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dims:\n",
    "    df_red, pca = reduce_embedding_dimensionality(df, d)\n",
    "    for p in pres:\n",
    "        df_pre = df_red.copy()\n",
    "        df_pre[f'Embedding - {d}'] = df_pre[f'Embedding - {d}'].apply(lambda x: x.astype(p))\n",
    "        for dis in ['Cosine', 'Euclidean', 'Manhattan']:\n",
    "            for prompt in (sp_prompts + sn_prompts):\n",
    "                prompt_embed = prompt_embeds[prompt]\n",
    "                prompt_embed = pca.transform(np.array(prompt_embed).reshape(1, -1))[0]\n",
    "\n",
    "                start = time.time()\n",
    "                distances = []\n",
    "                for i, row in df_pre.iterrows():\n",
    "                    distances.append(dist(dis, prompt_embed, row[f'Embedding - {d}']))\n",
    "\n",
    "                closest_indices = np.argsort(distances)\n",
    "\n",
    "                closest_rows = df_pre.iloc[closest_indices].values.tolist()\n",
    "\n",
    "                mfrs_returned = []\n",
    "                for m in [r[0] for r in closest_rows]:\n",
    "                    if m not in mfrs_returned:\n",
    "                        mfrs_returned.append(m)\n",
    "                # mfrs_found = [id for id in mfrs_returned if id in should_finds[prompt]]\n",
    "                # mfrs_new = [id for id in mfrs_returned if id not in should_finds[prompt]]\n",
    "                # mfrs_missed = [id for id in should_finds[prompt] if id not in mfrs_returned]\n",
    "                \n",
    "                mfrs_top_5 =  len([id for id in mfrs_returned[:5] if id in should_finds[prompt]]) / 5\n",
    "                mfrs_top_10 = len([id for id in mfrs_returned[:10] if id in should_finds[prompt]]) / 10\n",
    "                mfrs_top_20 = len([id for id in mfrs_returned[:20] if id in should_finds[prompt]]) / 20\n",
    "\n",
    "                end = time.time() - start\n",
    "                \n",
    "                df_to_concat = pd.DataFrame({\n",
    "                    'Dimensions' : [d],\n",
    "                    'Precision' : [str(p).split(\"'\")[1]],\n",
    "                    'Distance' : [dis],\n",
    "                    'Prompt' : [prompt],\n",
    "                    'Top 5' : [mfrs_top_5],\n",
    "                    'Top 10' : [mfrs_top_10],\n",
    "                    'Top 20' : [mfrs_top_20],\n",
    "                    'Time' : [round(end, 3)]\n",
    "                })\n",
    "                df_log = pd.concat([df_log, df_to_concat], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_grouped = df_log.drop(columns = ['Prompt']).groupby(['Dimensions', 'Precision', 'Distance']).agg({\n",
    "    'Top 5' : 'mean',\n",
    "    'Top 10' :'mean',\n",
    "    'Top 20' : 'mean',\n",
    "    'Time' : 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.to_csv('test_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_grouped.to_csv('test_log_grouped.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "055b01331e6aaed25ee87e2ba8c23bcef7463dead3293343036df99fa24c5bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
