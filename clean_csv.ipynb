{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mighty cleaner\n",
    "# I would love to document this properly some day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import time\n",
    "import hashlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SUBS_LEN = 2\n",
    "MAX_ALLOWED_RATE = 0.3\n",
    "MAX_ALLOWED_OCCURRENCES = 5\n",
    "\n",
    "\n",
    "def generate_id():\n",
    "    id = 1\n",
    "    while True:\n",
    "        yield str(id)\n",
    "        id += 1\n",
    "ID = generate_id()\n",
    "\n",
    "\n",
    "def sequence_is_allowed(seq_count, total_urls):\n",
    "    return seq_count <= MAX_ALLOWED_OCCURRENCES and seq_count < MAX_ALLOWED_RATE * total_urls\n",
    "\n",
    "\n",
    "def fill_segments_hashes(structure, segments_hash_values):\n",
    "    for segments_list in structure:\n",
    "        for seg in segments_list:\n",
    "            hash = hashlib.md5(seg.encode('utf-8')).hexdigest()\n",
    "            segments_hash_values[hash] = seg\n",
    "\n",
    "\n",
    "def fill_segments_two_way_map(structure, two_way_map):\n",
    "    for segments_list in structure:\n",
    "        for seg in segments_list:\n",
    "            if two_way_map.get(seg, None):\n",
    "                continue\n",
    "            seg_id = next(ID)\n",
    "            two_way_map[seg] = seg_id\n",
    "            two_way_map[seg_id] = seg\n",
    "\n",
    "\n",
    "def get_hashed_structure(structure):\n",
    "    return [ [ hashlib.md5(seg.encode('utf-8')).hexdigest() for seg in segment_list ] for segment_list in structure ]\n",
    "\n",
    "\n",
    "def get_original_struct_from_hashed(hashed_structure, segments_hash_values):\n",
    "    return [ [ segments_hash_values[hashed_seg] for hashed_seg in segment_list ] for segment_list in hashed_structure ]\n",
    "\n",
    "\n",
    "def swap_structure_state(encoded_structure, two_way_map):\n",
    "    return [ [ two_way_map[enc_seg] for enc_seg in segment_list ] for segment_list in encoded_structure ]\n",
    "\n",
    "\n",
    "def get_contigous_subsequences(segment_list):\n",
    "    return [segment_list[i:i+j] for i in range(0,len(segment_list)) for j in range(1,len(segment_list)-i+1)]\n",
    "\n",
    "\n",
    "def fill_contigous_subsequences(contigous_sequences, structure):    \n",
    "    for segments_list in structure:\n",
    "        for subs in get_contigous_subsequences(segments_list):\n",
    "            if len(subs) < MIN_SUBS_LEN:\n",
    "                continue\n",
    "            subsequence_token = ':'.join(subs)\n",
    "            contigous_sequences[subsequence_token] = contigous_sequences.get(subsequence_token, 0) + 1\n",
    "\n",
    "\n",
    "def clean_duplicate_segments(list_of_segments, contigous_sequences, total_urls):\n",
    "    # print('------------------')\n",
    "\n",
    "    size = len(list_of_segments)\n",
    "    \n",
    "    if size < MIN_SUBS_LEN:\n",
    "        return list_of_segments\n",
    "\n",
    "    window = MIN_SUBS_LEN\n",
    "    begin = 0\n",
    "    end = begin + window - 1\n",
    "\n",
    "    remove_list = []\n",
    "\n",
    "    while begin < size and end < size:\n",
    "        subsequence_token = ':'.join(list_of_segments[begin:end + 1])\n",
    "        # print('begin subs:', subsequence_token),\n",
    "        if not sequence_is_allowed(contigous_sequences.get(subsequence_token, 0), total_urls):\n",
    "            while end < size:\n",
    "                if end == size - 1:\n",
    "                    remove_list.append((begin, end))\n",
    "                    begin = end + 1\n",
    "                    end = begin + window - 1\n",
    "                    break\n",
    "                subsequence_token += ':' + list_of_segments[end+1]\n",
    "                # print('end subs:', subsequence_token)\n",
    "                if sequence_is_allowed(contigous_sequences.get(subsequence_token, 0), total_urls):\n",
    "                    # print('removing:', subsequence_token)\n",
    "                    remove_list.append((begin, end))\n",
    "                    begin = end + 1\n",
    "                    end = begin + window - 1\n",
    "                    break\n",
    "                end += 1\n",
    "        else:\n",
    "            begin += 1\n",
    "            end = begin + window - 1\n",
    "\n",
    "\n",
    "    # build the cleaned list (no duplicate sequences)\n",
    "    cleaned_list = []\n",
    "    i = 0\n",
    "    for range in remove_list:\n",
    "        if i < range[0]:\n",
    "            cleaned_list.extend(list_of_segments[i:range[0]])\n",
    "            i = range[1] + 1\n",
    "        else:\n",
    "            i = range[1] + 1\n",
    "    if i < size:\n",
    "        cleaned_list.extend(list_of_segments[i:])\n",
    "\n",
    "    # print(cleaned_list)\n",
    "    return cleaned_list\n",
    "            \n",
    "\n",
    "def clean_website_duplicate_sequences(structure):\n",
    "    # segments_hash_values = dict()\n",
    "    two_way_map = dict()\n",
    "    contigous_sequences = dict()\n",
    "    URL_COUNT = len(structure)\n",
    "\n",
    "    # fill_segments_hashes(structure, segments_hash_values)\n",
    "    fill_segments_two_way_map(structure, two_way_map)\n",
    "\n",
    "    # hashed_structure = get_hashed_structure(structure)\n",
    "    encoded_structure = swap_structure_state(structure, two_way_map)\n",
    "\n",
    "    fill_contigous_subsequences(contigous_sequences, encoded_structure)\n",
    "\n",
    "    # cleaned_hashed_structure = [ clean_duplicate_segments(list_of_segments, contigous_sequences, URL_COUNT) for list_of_segments in hashed_structure ]\n",
    "    # cleaned_structure = get_original_struct_from_hashed(cleaned_hashed_structure, segments_hash_values)\n",
    "    cleaned_encoded_structure = [ clean_duplicate_segments(list_of_segments, contigous_sequences, URL_COUNT) for list_of_segments in encoded_structure ]\n",
    "    \n",
    "    cleaned_structure = swap_structure_state(cleaned_encoded_structure, two_way_map)\n",
    "\n",
    "    return cleaned_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_similar(arrays, urls, thresh = 0.95):\n",
    "    # Create a new list to store non-duplicate arrays and corresponding URLs\n",
    "    non_duplicate_arrays = []\n",
    "    non_duplicate_urls = []\n",
    "    # Iterate over each array and URL in the input lists\n",
    "    for i, (array1, url1) in enumerate(zip(arrays, urls)):\n",
    "        # Create a new set to store unique strings in the current array\n",
    "        unique_strings1 = set(array1)\n",
    "        # Iterate over each array and URL that come after the current array and URL in the lists\n",
    "        for j in range(i+1, len(arrays)):\n",
    "            array2 = arrays[j]\n",
    "            url2 = urls[j]\n",
    "            # Calculate the number of common strings between the two arrays\n",
    "            common_strings = set(array2).intersection(unique_strings1)\n",
    "            try:\n",
    "                similarity = len(common_strings) / len(unique_strings1)\n",
    "            except ZeroDivisionError:\n",
    "                similarity = 0\n",
    "            # If the similarity is 95% or greater, remove both arrays and URLs\n",
    "            if similarity >= thresh:\n",
    "                break\n",
    "        else:\n",
    "            non_duplicate_arrays.append(array1)\n",
    "            non_duplicate_urls.append(url1)\n",
    "    return non_duplicate_arrays, non_duplicate_urls\n",
    "\n",
    "\n",
    "def remove_duplicates(array_of_arrays):\n",
    "    result = []\n",
    "    for inner_array in array_of_arrays:\n",
    "        unique_elements = []\n",
    "        for element in inner_array:\n",
    "            if element.lower() not in unique_elements:\n",
    "                unique_elements.append(element.lower())\n",
    "        result.append(unique_elements)\n",
    "    return result\n",
    "\n",
    "def clean_lists(lists, thresh = 3):\n",
    "    all = []\n",
    "    for i in lists:\n",
    "        all.extend(i)\n",
    "    \n",
    "    counts = {}\n",
    "    duplicates = set()\n",
    "\n",
    "    for string in all:\n",
    "        if string in counts:\n",
    "            counts[string] += 1\n",
    "            if counts[string] >= thresh:\n",
    "                duplicates.add(string)\n",
    "        else:\n",
    "            counts[string] = 1\n",
    "\n",
    "    total = []\n",
    "    for x in lists:\n",
    "        temp = []\n",
    "        for y in x:\n",
    "            if y not in duplicates:\n",
    "                temp.append(y)\n",
    "        total.append(temp)\n",
    "    \n",
    "    return total\n",
    "\n",
    "def remove_words(array_of_arrays):\n",
    "    result = []\n",
    "    removed = []\n",
    "    for inner_array in array_of_arrays:\n",
    "        arrResult = []\n",
    "        arrRemoved = []\n",
    "        for element in inner_array:\n",
    "            if len(element.split()) > 1 and non_letter_ratio(element) < 0.4:\n",
    "                arrResult.append(element)\n",
    "            else:\n",
    "                arrRemoved.append(element)\n",
    "        result.append(arrResult)\n",
    "        removed.append(arrRemoved)\n",
    "    return result, removed\n",
    "    \n",
    "def non_letter_ratio(string):\n",
    "    text = ''.join(string.split())\n",
    "    special_characters = '\"\\'!@#$%^&*()-+?_=,<>/\"\\\\|'\n",
    "    up = 0\n",
    "    down = len(text)\n",
    "    for char in text:\n",
    "        if char in special_characters or char.isdigit():\n",
    "            up += 1\n",
    "    return up/down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner without remove similar, looping over df using index\n",
    "def cleaner(input_df):\n",
    "    input_df['Cleaned'] = None\n",
    "\n",
    "    segments = input_df['Segments'].values.tolist()\n",
    "    pages_list = input_df['Page'].values.tolist()\n",
    "\n",
    "    dedupeArr = remove_duplicates(segments)\n",
    "    cleaned = clean_website_duplicate_sequences(dedupeArr)\n",
    "    results, removed  = remove_words(cleaned)\n",
    "\n",
    "    counter = 0\n",
    "    input_df = input_df.reset_index()\n",
    "    for r in results:\n",
    "        input_df.at[counter, 'Cleaned'] = r\n",
    "        counter += 1\n",
    "    return input_df.drop(columns = ['index'])\n",
    "\n",
    "\n",
    "\n",
    "def cleaner(input_df):\n",
    "    input_df['Cleaned'] = None\n",
    "\n",
    "    segments = input_df['Segments'].values.tolist()\n",
    "    pages_list = input_df['Page'].values.tolist()\n",
    "\n",
    "    dedupeArr = remove_duplicates(segments)\n",
    "    cleaned = clean_website_duplicate_sequences(dedupeArr)\n",
    "    newTexts, newUrls = remove_similar(cleaned, pages_list)\n",
    "    results, removed  = remove_words(newTexts)\n",
    "\n",
    "    input_df = input_df.set_index('Page')\n",
    "    for u, r in zip(newUrls, results):\n",
    "        input_df.at[u, 'Cleaned'] = r\n",
    "    input_df = input_df.reset_index()[['ID', 'Page', 'Segments', 'Cleaned']]\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(sorted(os.listdir('Segmented By ID'), key=len)):\n",
    "    print(i+1, end = '\\t\\t')\n",
    "    print('ID: ', str(f.rstrip('.csv')).ljust(10), end = '')\n",
    "    start = time.time()\n",
    "    new_df = pd.read_csv(f'Segmented By ID/{f}', index_col=False)\n",
    "    new_df.drop_duplicates(subset='Page', keep='first', inplace=True)\n",
    "    new_df['Segments'] = new_df['Segments'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    page_count = len(new_df)\n",
    "    print(str(page_count).ljust(10), end = '')\n",
    "    seg_count = sum([len(row) for row in new_df['Segments']])\n",
    "    print(str(seg_count).ljust(15), end = '')\n",
    "\n",
    "    if f in os.listdir('Cleaned By ID'):\n",
    "        print('ALREADY CLEANED')\n",
    "        continue\n",
    "\n",
    "    if page_count < 3 or page_count >= 500 or (f.rstrip('.csv') in ['1183', '25997', '29935', '284418']):\n",
    "        print('SKIPPED')\n",
    "        continue\n",
    "\n",
    "    new_df = cleaner(new_df)\n",
    "    new_df.to_csv(f'Cleaned By ID/{f}', index = False)\n",
    "    print(time.time() - start, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "055b01331e6aaed25ee87e2ba8c23bcef7463dead3293343036df99fa24c5bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
