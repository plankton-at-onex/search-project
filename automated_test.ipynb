{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook automates search for multiple prompts, and generates a df with the results for each search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import math\n",
    "import time\n",
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    response = openai.Embedding.create(\n",
    "    input=input,\n",
    "    model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "    \n",
    "def dist(arr2, arr1):\n",
    "    # Given that arr1 is a np array of lists, and arr2 is a np array\n",
    "    return np.dot(arr1,arr2)/(norm(arr1, axis=1)*norm(arr2))\n",
    "\n",
    "def dist2(arr1, arr2):\n",
    "    # Given that arr1 np array, and arr2 is a np array\n",
    "    return np.dot(arr1,arr2)/(norm(arr1)*norm(arr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sheets(df, info):\n",
    "\n",
    "    # Create page count df\n",
    "    id_count = df.groupby('ID').count().to_dict()['Embedding']\n",
    "    df = pd.DataFrame([id_count]).transpose().reset_index()\n",
    "    df.columns = ['ID', 'Count']\n",
    "\n",
    "    info = info[info['ID'].isin(id_count.keys())]\n",
    "    info = info.reset_index().drop(columns=['index'])\n",
    "\n",
    "    pages_df = pd.merge(info, df, how ='inner', on = 'ID')\n",
    "    pages_df.drop(pages_df[pages_df['Count'] == 0].index)\n",
    "    pages_df['Scientific Name'] = pages_df['Scientific Name'].apply(lambda x: x.split(', '))\n",
    "    pages_df['Specialty'] = pages_df['Specialty'].apply(lambda x: x.split(', '))\n",
    "    \n",
    "    # count occurrences of Scientific Name\n",
    "    scientific_name_counts = {}\n",
    "    for names in pages_df['Scientific Name']:\n",
    "        for name in names:\n",
    "            if name not in scientific_name_counts:\n",
    "                scientific_name_counts[name] = 1\n",
    "            else:\n",
    "                scientific_name_counts[name] += 1\n",
    "\n",
    "    # count occurrences of Specialty\n",
    "    specialty_counts = {}\n",
    "    for specialties in pages_df['Specialty']:\n",
    "        for specialty in specialties:\n",
    "            if specialty not in specialty_counts:\n",
    "                specialty_counts[specialty] = 1\n",
    "            else:\n",
    "                specialty_counts[specialty] += 1\n",
    "\n",
    "    # create new dataframes with the counts\n",
    "    scientific_name_df = pd.DataFrame.from_dict(scientific_name_counts, orient='index', columns=['Count'])\n",
    "    specialty_df = pd.DataFrame.from_dict(specialty_counts, orient='index', columns=['Count'])\n",
    "\n",
    "    # add column to indicate specialty or scientific name\n",
    "    scientific_name_df['Type'] = 'Scientific Name'\n",
    "    specialty_df['Type'] = 'Specialty'\n",
    "\n",
    "    # merge the two dataframes\n",
    "    sp_sn_count = pd.concat([scientific_name_df, specialty_df], axis=0, sort=False)\n",
    "    sp_sn_count = sp_sn_count.reset_index()\n",
    "    sp_sn_count.columns = ['Word', 'Count', 'Type']\n",
    "    sp_sn_count = sp_sn_count.sort_values('Count', ascending=False)\n",
    "    \n",
    "    return pages_df, sp_sn_count.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('openai_concat_embed_sample_smaller.csv')\n",
    "df = df.dropna(subset=['Embedding'])\n",
    "df['Embedding'] = df['Embedding'].apply(lambda x: \n",
    "                                    np.fromstring(\n",
    "                                        x.replace('[','')\n",
    "                                        .replace(']','')\n",
    "                                        .replace('  ',' '), sep=', '))\n",
    "\n",
    "info , test_cases = generate_sheets(df, pd.read_csv('Google Sheets.csv'))\n",
    "info = info.set_index('ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_test(test_list, n = None, threshold=0.8, n_chars=0, details = False):\n",
    "    for prompt in test_list:\n",
    "        # Retreive the IDs of the MFRs that have the prompt as a scientific name or specialty\n",
    "        should_find = info[info['Specialty'].apply(lambda x: prompt in x)].index.tolist()\n",
    "        should_find.extend(info[info['Scientific Name'].apply(lambda x: prompt in x)].index.tolist())\n",
    "        should_find = list(set(should_find))\n",
    "        \n",
    "        # Set n to be 20% over the number of ids that belong to the prompt as a scientific name, or as a prompt.\n",
    "        if n == None:\n",
    "            top = math.ceil(1.20 * len(should_find))\n",
    "\n",
    "        # Embed the prompt\n",
    "        prompt_embed = embed(prompt)\n",
    "\n",
    "        # Calculate cosine distances between prompt and each product\n",
    "        distances = []\n",
    "        for i, row in df.iterrows():\n",
    "            distances.append(dist2(prompt_embed, row['Embedding']))\n",
    "\n",
    "        # Sort the products by distance in descending order\n",
    "        closest_indices = np.argsort(distances)[::-1]\n",
    "        closest_rows = df.iloc[closest_indices].values.tolist()\n",
    "        closest_distances = [distances[i] for i in closest_indices]\n",
    "\n",
    "        # Initialize variables for tracking manufacturer scores\n",
    "        manufacturer_scores = {}\n",
    "        manufacturer_pages = {}\n",
    "\n",
    "        # Iterate over the products\n",
    "        for r, distance in zip(closest_rows, closest_distances):\n",
    "            # Extract the manufacturer ID\n",
    "            id = r[1]\n",
    "\n",
    "            # If the distance is below the threshold, skip this product\n",
    "            if distance < threshold:\n",
    "                continue\n",
    "\n",
    "            # If we haven't seen this manufacturer yet, initialize their score and page list\n",
    "            if id not in manufacturer_scores:\n",
    "                manufacturer_scores[id] = 0\n",
    "                manufacturer_pages[id] = []\n",
    "\n",
    "            # Add the distance to the manufacturer's score and add the page to their list\n",
    "            if len(r[3]) > n_chars:\n",
    "                manufacturer_scores[id] += distance\n",
    "                manufacturer_pages[id].append((r[2], r[3], distance))\n",
    "\n",
    "        # Sort the manufacturers by score in descending order\n",
    "        sorted_manufacturers = sorted(manufacturer_scores.items(), key=lambda x: x[1], reverse=True)[:top]\n",
    "        manufacturer_returned = [x[0] for x in sorted_manufacturers]\n",
    "\n",
    "        manufacturers_found = [id for id in manufacturer_returned if id in should_find]\n",
    "        manufacturer_new = [id for id in manufacturer_returned if id not in should_find]\n",
    "        manufacturers_missed = [id for id in should_find if id not in manufacturer_returned]\n",
    "\n",
    "        print('Prompt:', prompt)\n",
    "        print(f'\\n# of Manufacturers with prompt as SN/SP: {len(should_find)}')\n",
    "        if details:\n",
    "            for m in should_find:\n",
    "                name = info.loc[m, 'Manufacturer']\n",
    "                print(f'\\tID: {str(m).ljust(10)} Name:{name}')\n",
    "            \n",
    "        \n",
    "        if len(should_find) > 0:\n",
    "            print(f'\\n# of Missed Manufacturers: {len(manufacturers_missed)} / {len(should_find)} --> {round((len(manufacturers_missed) / len(should_find) * 100))}%')\n",
    "            if details:\n",
    "                for m in manufacturers_missed:\n",
    "                    name = info.loc[m, 'Manufacturer']\n",
    "                    print(f'\\tID: {str(m).ljust(10)} Name:{name}')\n",
    "\n",
    "            print(f'\\n# of Manufacturers successfully found: {len(manufacturers_found)} / {len(should_find)} --> {round((len(manufacturers_found) / len(should_find) * 100))}%')\n",
    "            if details:\n",
    "                for i, m in enumerate(manufacturers_found):\n",
    "                    name = info.loc[m, 'Manufacturer']\n",
    "                    url = info.loc[m, 'Website']\n",
    "                    sn = info.loc[m, 'Scientific Name']\n",
    "                    sp = info.loc[m, 'Specialty']\n",
    "\n",
    "                    print(f'\\n\\t--- {i+1} --- ')\n",
    "                    print(f'\\t\\tMFR Name: {name}')\n",
    "                    print(f'\\t\\tMFR ID: {id}')\n",
    "                    print(f'\\t\\tMFR URL: {url}')\n",
    "                    print(f'\\t\\tMFR SN: {sn}')\n",
    "                    print(f'\\t\\tMFR Specialty: {sp}')\n",
    "                    print('\\n\\t\\t\\tPages:')\n",
    "                    for page in manufacturer_pages[m]:\n",
    "                        print(f'\\t\\t\\t\\t Cosine Similarity: {page[2]}')\n",
    "                        print(f'\\t\\t\\t\\t URL: {urllib.parse.unquote(page[0])}')\n",
    "                        print(f'\\t\\t\\t\\t--- Segment: {page[1]}')\n",
    "                    print('\\n\\n')\n",
    "\n",
    "        print(f'\\n# of New Manufacturers: {len(manufacturer_new)} / {len(manufacturer_returned)}')\n",
    "        if details:\n",
    "            for i, m in enumerate(manufacturer_new):\n",
    "                name = info.loc[m, 'Manufacturer']\n",
    "                url = info.loc[m, 'Website']\n",
    "                sn = info.loc[m, 'Scientific Name']\n",
    "                sp = info.loc[m, 'Specialty']\n",
    "\n",
    "                print(f'\\n\\t--- {i+1} --- ')\n",
    "                print(f'\\t\\tMFR Name: {name}')\n",
    "                print(f'\\t\\tMFR ID: {id}')\n",
    "                print(f'\\t\\tMFR URL: {url}')\n",
    "                print(f'\\t\\tMFR SN: {sn}')\n",
    "                print(f'\\t\\tMFR Specialty: {sp}')\n",
    "                print('\\n\\t\\t\\tPages:')\n",
    "                for page in manufacturer_pages[m]:\n",
    "                    print(f'\\t\\t\\t\\t Cosine Similarity: {page[2]}')\n",
    "                    print(f'\\t\\t\\t\\t URL: {urllib.parse.unquote(page[0])}')\n",
    "                    print(f'\\t\\t\\t\\t--- Segment: {page[1]}')\n",
    "                print('\\n\\n')\n",
    "        print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_test_df(test_list, n = None, threshold=0.8, n_chars=0, details = False):\n",
    "    output_df = pd.DataFrame(columns = ['Prompt', 'Type', 'Should_Find', 'Returned', 'Found', 'Missed', 'New'])\n",
    "    for i, prompt in enumerate(test_list):\n",
    "        print(i, end = '\\t')\n",
    "        # Retreive the IDs of the MFRs that have the prompt as a scientific name or specialty\n",
    "        should_find = info[info['Specialty'].apply(lambda x: prompt in x)].index.tolist()\n",
    "        should_find.extend(info[info['Scientific Name'].apply(lambda x: prompt in x)].index.tolist())\n",
    "        should_find = list(set(should_find))\n",
    "        \n",
    "        # Set n to be 20% over the number of ids that belong to the prompt as a scientific name, or as a prompt.\n",
    "        if n == None:\n",
    "            top = math.ceil(1.20 * len(should_find))\n",
    "\n",
    "        # Embed the prompt\n",
    "        prompt_embed = embed(prompt)\n",
    "\n",
    "        # Calculate cosine distances between prompt and each product\n",
    "        distances = []\n",
    "        for i, row in df.iterrows():\n",
    "            distances.append(dist2(prompt_embed, row['Embedding']))\n",
    "\n",
    "        # Sort the products by distance in descending order\n",
    "        closest_indices = np.argsort(distances)[::-1]\n",
    "        closest_rows = df.iloc[closest_indices].values.tolist()\n",
    "        closest_distances = [distances[i] for i in closest_indices]\n",
    "\n",
    "        # Initialize variables for tracking manufacturer scores\n",
    "        manufacturer_scores = {}\n",
    "        manufacturer_pages = {}\n",
    "\n",
    "        # Iterate over the products\n",
    "        for r, distance in zip(closest_rows, closest_distances):\n",
    "            # Extract the manufacturer ID\n",
    "            id = r[1]\n",
    "\n",
    "            # If the distance is below the threshold, skip this product\n",
    "            if distance < threshold:\n",
    "                continue\n",
    "\n",
    "            # If we haven't seen this manufacturer yet, initialize their score and page list\n",
    "            if id not in manufacturer_scores:\n",
    "                manufacturer_scores[id] = 0\n",
    "                manufacturer_pages[id] = []\n",
    "\n",
    "            # Add the distance to the manufacturer's score and add the page to their list\n",
    "            if len(r[3]) > n_chars:\n",
    "                manufacturer_scores[id] += distance\n",
    "                manufacturer_pages[id].append((r[2], r[3], distance))\n",
    "\n",
    "        # Sort the manufacturers by score in descending order\n",
    "        sorted_manufacturers = sorted(manufacturer_scores.items(), key=lambda x: x[1], reverse=True)[:top]\n",
    "        manufacturer_returned = [x[0] for x in sorted_manufacturers]\n",
    "\n",
    "        manufacturers_found = [id for id in manufacturer_returned if id in should_find]\n",
    "        manufacturer_new = [id for id in manufacturer_returned if id not in should_find]\n",
    "        manufacturers_missed = [id for id in should_find if id not in manufacturer_returned]\n",
    "        if prompt in pd.unique(info['Specialty'].explode()).tolist():\n",
    "            temp_df = pd.DataFrame({'Prompt':[prompt], 'Type':['Specialty'], 'Should_Find': [len(should_find)],'Returned': [len(manufacturer_returned)],\n",
    "            'Found':[len(manufacturers_found)], 'Missed':[len(manufacturers_missed)], 'New':[len(manufacturer_new)]})\n",
    "        else:\n",
    "            temp_df = pd.DataFrame({'Prompt':[prompt], 'Type':['Scientific Name'], 'Should_Find': [len(should_find)],'Returned': [len(manufacturer_returned)],\n",
    "            'Found':[len(manufacturers_found)], 'Missed':[len(manufacturers_missed)], 'New':[len(manufacturer_new)]})\n",
    "        output_df = pd.concat([output_df, temp_df], ignore_index=True)\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all scientific names and specialities from sample\n",
    "# all_sp = pd.unique(info['Specialty'].explode()).tolist()\n",
    "# all_sn = pd.unique(info['Scientific Name'].explode()).tolist()\n",
    "# to_test = all_sp\n",
    "# to_test.extend(all_sn)\n",
    "# to_test = list(set(to_test))\n",
    "\n",
    "# get top 30 scientific names and specialties\n",
    "to_test = [x[0] for x in test_cases[['Word', 'Count']].values.tolist()][:30]\n",
    "# to_test = [x[0] for x in test_cases[['Word', 'Count', 'Type']].values.tolist() if x[2] == 'Scientific Name'][:30]\n",
    "\n",
    "out_sp = automated_test_df(to_test, threshold=0.8, n_chars=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "055b01331e6aaed25ee87e2ba8c23bcef7463dead3293343036df99fa24c5bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
